---
title: An R Markdown document converted from "quickstart-amazon-baby-reviews-bda2021.ipynb"
output: html_document
---

```{r}
## Importing packages

library(tidyverse) # metapackage with lots of helpful functions
library(tidytext)

## Data attached to this notebook
KAGGLE_RUN <- FALSE
if (KAGGLE_RUN) list.files(path = "../input")
```

# 1. The project

In this competition you will predict customer sentiments regarding [Baby products purchased on Amazon.com](http://jmcauley.ucsd.edu/data/amazon/), on the basis of their written reviews. 

Answer the 3 most important questions for any ML project:

1. Where do the data come from? (To which population will results generalize?)
2. What are candidate machine learning methods? (models? features?)
3. What is the Bayes' error bound?

<div style=color:darkblue;background-color:#fafaff;min-height:8em; >


<br>
<em>... Double tap to type your team's answers to all three questions here ...</em>
<br>



</div>

# 2. Read Data

Locate and load the data into memory.

```{r}
dir("../input", recursive=TRUE)
```

```{r}
# Find the right file path
if (KAGGLE_RUN) csv_filepath = dir("..", pattern="amazon_baby.csv", recursive=TRUE, full.names = TRUE) else csv_filepath = "amazon_baby_subset.csv"

# Read in the csv file
amazon = read_csv(csv_filepath) %>%
    rownames_to_column('id') 
```

```{r}
head(amazon)
```

The data frame contains both the train and test data. The test data are the reviews for which the rating is missing and you need to provide a prediction. 

The following logical index variable will be of help selecting the desired rows without the need to split the data frame into seperate sets. This makes it easier to extract features.

```{r}
trainidx = !is.na(amazon$rating)
table(trainidx)
```

From the above, there are 153,531 training samples and 30,000 test samples.

# 3. Preprocessing

The table contains, the product `name`, the textual `review`, and the `rating`. Should we use only the `review`'s or also the `name`? Clearly the products differ on quality which will cause the ratings to differ, so we want to include product identiy as a predictive feature. How can we do this? Well, there are several ways, but we'll use a trick: we'll prepend the product name to the review text. That is we will paste the `name` string and `review` string into a single string. In that way, we incorporate both product names and review text without haveing to handle product names separately. 

Doing so will also handle another problem: Some of the reviews that are empty strings, and we wont be able to make a prediction for them. By pasting `name` and `review` we'll at least have the name to predict their rating. 

Here is code for pasting `name` and `review` using the `unite()` function:

```{r}
# Paste name and review into a single string separated by a "–".
# The new string replaces the original review.
amazon = amazon %>% 
    unite(review, name, review, sep = " — ", remove = FALSE)

print(amazon)
```

# 3.1 Tokenization

We're going to use tidytext to break up the text into separate tokens and count the number of occurences per review. To keep track of the review to which the review belongs, we have added the rownames as `id` above, which is simply the row number. As tokens you can consider single words, pairs of words called bi-grams, or n-grams. 

```{r}
reviews = amazon %>% 

   # sample_n(5000) %>% # start with smallish sample; for you final model use all data!

   # tokinize reviews at word level
   unnest_tokens(token, review) %>%

   # count tokens within reviews as 'n'
   # (keep id, name, and rating in the result)
   count(id, name, rating, token)

head(reviews)
```

## Stopwords

A common step in text analytics is to remove unimportant words. But what are unimportant words? Linguists have come up with lists of so called stop words that we've run into before. Words on these lists, such as 'me', 'what', 'a', 'the', etcetera, putatively do not carry a lot of information. This is because they occur so frequently in any text, but especially because their usage does not vary a lot across texts on different topics.

However, you should always ask yourself "information on what specifically?". In the Personality competion you may have observed that words like 'me', 'myself', 'we', do seem to carry information about personality. Hence, habitual stop word removal (that you'll find advocated in online posts all the time) is generally ill advised.

A priori we mostly have little reason to assume any potential feature is uninformative with respect to the target variable, and the only way to find out, is to test the hypothesis statistically. For instance, if we look specifically whether the stop words listed in the table returned by `get_stopwords()`, we find that their counts are highly predictive for `rating`, even if we control for stop word totals per review:

```{r}
sw = reviews %>% 

  # Add the total number of tokens per review as 'N'
  add_count(id, name = "N") %>% 

  # Retain only tokens that are stopwords
  inner_join(get_stopwords(), by = c(token='word')) %>% 

  # Compute the total number of stopwords per review
  group_by(id, rating, N) %>% 
  summarise(n_stopwords = sum(n))

head(sw)
```

The resulting data frame allows us to relate `rating` to the number stopwords in the review, and check if this relation is statistical significant. To make sure the relation isn't caused by the total number of tokens `N`, we correct for it in the regression:

```{r}
lm(rating ~ n_stopwords + N, sw) %>% summary()
```

The regression coefficients are highly significant, even thought the explained variance is rather small. Individual words like "a" and "what" are also highly significant. 

__Note__: This should not be perceived in any way as an encouragement to conduct regression analyses for all kinds of text features&mdash;there are simply too many of them. It is just to demonstrate that you shouldn't mindlessly follow common praxis or convention in removing "uninformative" features. 

The only reason to mention stopwords here is because too often it prescribed as non-optional in internet posts. It is best to rely on non-zero variance and correlations (and multicolinearity) to decide on "non-informative features").


# 4. Features engineering

Features computed for tokens in text are based on the Bag of Words (BoW) model: Each document is considered a bag of words, in wich order plays no particular. A better name would be Bag of Tokens, because tokens can also be bi-grams etc, but we're stuck with BoW. Common features are


- **document occurence**: 
    > 0-1 encoding of the presence or absence of a token in a document (here: review)
    
- **token counts**: 
    > simple counts $n_{t,d}$ of each token $t$ within documents $d$ (resulting in a document by term matrix, or DTM)

- **term frequency ($TF_{d,t}$)**: 
    > the relative frequency of a term within a document $\displaystyle {n_{d,t} \over  \sum_t n_{d,t}}$

- **inverse document frequency ($IDF_t$)**: 
    > inverse the relative frequency with which a term occurs among the $N$ documents, expressed on a log scale (a measure of 'surprise') as  $-\log\left({DF_t \over N}\right)$ Here $DF_t$ is the number of documents that contain the token $t$.

- **the $TFIDF_{d,t}$**: 
    > the product of TF and IDF

- **vector space embeddings**: 
    > advanced features like factor loadings (eigen vectors) from a PCA of the DTM, or "word2vec" representations of words, sentences, and paragraphs (not discussed here), usually obtained by training neural networks on a very large corpus


The motivation for $TF_{d,t}$ is simply that the more often a token $t$ occurs in a document, the more likely it is that the topic of the document is closely related to that token. A problem of $TF_{d,t}$ is that it does not take into account that certain words simply occur more frequently because of their role in language (such as 'a', 'but', etc.). 

The motivation for the $IDF_t$ is that the more wide spread the use of a token $t$ is among all documents, the less likely it conveys information about the topic of any particular document. Hence, the more surprising a word is, the more likely it conveys information about the topic of the document in which it is found. 

The $TFIDF_{d,t}$ banks on both of these ideas and quantifies the important of a term for a given document. 

While $TFIDF_{d,t}$ is extensively applied and very successful in document retrieval systems (i.e., search engines), the $IDF_t$ part has much less use over $TF_{d,t}$ in *predictive* models because the $IDF_t$ part simply scales the $TF_{d,t}$ features accross documents. This scaling may have an effect on scale sensitive algorithms like PCA and algorithms that rely on Euclidean distances such as kNN. 

(Btw: While linear and logistic regression are scale insensitive because they can absorb scale differences in the regression coefficients, LDA and QDA are insenstive to scaling because the compute Mahalanobis distance and not Euclidean distance. The Mahalanobis distance is the Euclidean distance after standardizing in SVD space.)

```{r}
## Compute token features

```

## Non-zero variance features

Features that have almost no variance across cases cannot provide a lot of information about the target variable. Variance across cases is the leading principle in any data context. For binary and count data as considered here the variance is determined by the average (that's a mathetmatical fact). Hence, for the current data we can look simply at document frequencies and do not need to compute variances. 

We will remove tokens that occur in less than 0.01% of the documents (there are ~180,000 reviews in the data set; less than 0.01% &times; 180,000 reviews = 18 of the reviews). The number 0.01% is quite arbitrary, but will remove idiosyncratic strings and miss-spellings that occur only in singular reviews. 

Since $IDF_t$, the column `idf`, which measures the surprise of a `token` $t$, is computed as 

$$IDF_t = -\log\left({\text{df}_t \over N}\right) = -\log(\text{proportion of document in which }t\text{ occurs})$$ 

we can filter the rows in `features` for which $-\log(\text{df}_t / N) \leq -\log(0.01\%)$ (i.e., the 'surprise' should be lower than $-\log(0.01/100)$).


```{r}
# Near-zero variance features


```

## Correlated features

Although correlated features may exist, with thousands of features it's computationally too cumbersome to try to remove them directly. Instead we'll have to rely on the properties of the Lasso and Ridge regression to deal with them (look it up in the ISLR book; it might come up in an exam question).



# 5. Models

## Not relying on manual feature selection

In the Personality competition we computed features by utilizing word lists that in previous research were found to be predictive of sentiment. This requires substantial input from experts on the subject. If such knowledge is not (yet) available a process of trial and error can be used. But with many thousands of features automation of this process is essential. 


In addition forward and/or backward selection, automated methods that try to automatically ballance flexibility and predictive performance are

1. Lasso and Ridge regression
2. Principal Components and Partial Least Squares regression
3. Smoothing 
4. Regression and Classification trees (CART)
5. Random Forests
6. Support Vector Machines

Methods (1) and (2) on this list involve methods are able to take many features while automatically reducing redundant flexibility to any desired level. Multicollinearity, the epithome of reduancy, is also automatically taken care of by these methods.

Number (3) on the list, smoothing, grants more flexibility by allowing for some non-linearity in the relations between features and the target variable, without the need to manually specify a specific mathematical form (as is necessary in polynomial regression).

Methods (4), (5), and (6) are not only able to remove redundant features, but also can automatically recognize interactions between  features.

Hence, all of these methods remove the necessity of finding the best features by hand. 

All of these methods are associated with a small set of 1 to 3 (or 4 in some cases) parameters that control the flexibility of the model in a more or less continuous way&mdash;much like the $k$ parameter in k-nearest neighbers. Like the $k$ parameter in k-NN, these parameters can and need to be adjusted (*'tuned'*) for optimal predictive performance. Tuning is best done on a validation set (a subset from the training data), or using cross-validation, depending on the size of the data set.

# 5.1 Model fitting

Not all algorithms can deal with sparse matrices. For instance `lm()` can't. The package `glmnet`, which is extensively discussed in chapter 6 of ISLR, has a function with the same name `glmnet()` which can handle sparse matrices, and also allow you to reduce the model's flexibility by means of the Lasso penalty or ridge regression penalty. Furthermore, like the standard `glm()` function, it can also handle a variety of dependent variable families, including gaussian (for linear regression), binomial (for logistic regression), multinomial (for multinomial logistic regression), Poisson (for contingency tables and counts), and a few others. It is also quite caple of dealing computationally efficiently with the many features we have here.

> <span style=color:brown>The aim of this competition is the predict the probability that a customer is ***satisfied***. This is deemed to be the case if `rating > 3`.  Hence, you will need as a dependent variable `y` a factor that specifies whether this is the case. </span>

The performance of your submission will be evaluated using the area under the curve (AUC) of the receiver operating curve (ROC). See chapter 4 in the ISLR book. See also the help file for how `cv.glmnet` can works with this measure.

As said, `glmnet()` allows you to tune the flexibility of the model by means of _regularizing_ the regression coefficients. The type of regularization (i.e., the Lasso or ridge) that is used is controled by the `alpha` parameter. Refer to the book for an explanation. The amount of regularization is specified by means of the `lambda` parameter. Read the warning in the `help(glmnet)` documentation about changing this parameter. To tune this parameter look at the `cv.glmnet()` function.

```{r}
## Prepare features for glmnet


# Features to design matrix


# Target variable

```

```{r}
# Split design matrix and target into training and test portions

```

```{r}
# Fit model

```

# 5.2 Model evaluation


To evaluate the model you can look at various predictive performance measures. Given that AUC is the performance measure used to rate your submission in this competition, it is of special importance. But other performance indicators are interesting to look at too. Consider tabulating and/or graphing performance differences from tuning and different models.

Try to understand what the model does, and consider drawing some conclusions.

```{r}
# Performance evaluation

```

```{r}
# Model comparisons


# Insight into model behavior
```


# 6. Submitting your predictions

A sample file is provided to make sure that you predict the right cases and submit your predictions in the right format:

```{r}
sample_filepath = dir("..", pattern="amazon_baby_testset_sample.csv", recursive=TRUE, full.names = TRUE)

sample_submission = read_csv(sample_filepath, col_types = cols(col_character(), col_double()))

head(sample_submission)
```

```{r}

```

